{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red COLOR\n",
      "blue COLOR\n",
      "green COLOR\n",
      "yellow COLOR\n",
      "orange COLOR\n",
      "purple COLOR\n",
      "pink COLOR\n",
      "brown COLOR\n",
      "black COLOR\n",
      "white COLOR\n",
      "gray COLOR\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "\n",
    "# Create a blank language model for English\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Add a Named Entity Recognition (NER) component to the pipeline\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "ner.add_label(\"COLOR\")  # Add the \"COLOR\" label to the NER component\n",
    "\n",
    "# Training data: list of sentences with labeled entities\n",
    "training_data = [\n",
    "    (\"The apple is red\", {\"entities\": [(12, 15, \"COLOR\")]}),\n",
    "    (\"The door is blue\", {\"entities\": [(11, 15, \"COLOR\")]}),\n",
    "    (\"The car is green\", {\"entities\": [(11, 16, \"COLOR\")]}),\n",
    "    (\"The house is yellow\", {\"entities\": [(13, 19, \"COLOR\")]}),\n",
    "    (\"The stadium is orange\", {\"entities\": [(15, 21, \"COLOR\")]}),\n",
    "    (\"The app is purple\", {\"entities\": [(11, 17, \"COLOR\")]}),\n",
    "    (\"The ball is pink\", {\"entities\": [(11, 15, \"COLOR\")]}),\n",
    "    (\"The shirt is brown\", {\"entities\": [(12, 17, \"COLOR\")]}),\n",
    "    (\"The cat is black\", {\"entities\": [(11, 16, \"COLOR\")]}),\n",
    "    (\"The book is white\", {\"entities\": [(12, 17, \"COLOR\")]}),\n",
    "    (\"The cloud is gray\", {\"entities\": [(12, 16, \"COLOR\")]}),\n",
    "    (\"The bike is silver\", {\"entities\": [(12, 18, \"COLOR\")]}),\n",
    "]\n",
    "# Convert the training data into Spacy's format\n",
    "spacy_training_data = []\n",
    "for text, annotations in training_data:\n",
    "    example = Example.from_dict(nlp.make_doc(text), annotations)\n",
    "    spacy_training_data.append(example)\n",
    "\n",
    "# Disable other pipeline components during training to only update the NER component\n",
    "disabled_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "\n",
    "# Training loop\n",
    "with nlp.disable_pipes(*disabled_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "    for iteration in range(100):  # Number of iterations\n",
    "        losses = {}\n",
    "        for example in spacy_training_data:\n",
    "            nlp.update([example], drop=0.5, losses=losses)\n",
    "        # Uncomment the following line to print losses during training\n",
    "        # print(losses)\n",
    "\n",
    "# Save the trained model to a file\n",
    "nlp.to_disk(\"color_ner_model\")\n",
    "\n",
    "# Load the trained model\n",
    "nlp_loaded = spacy.load(\"color_ner_model\")\n",
    "\n",
    "# Example text for entity recognition\n",
    "text = \"red blue green yellow orange purple pink brown black white gray\"\n",
    "\n",
    "# Process the text using the trained model\n",
    "doc = nlp_loaded(text)\n",
    "\n",
    "# Extract recognized entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.94591        0.200\n",
      "             2          -0.71757        1.000\n",
      "             3          -0.45197        1.000\n",
      "             4          -0.32966        1.000\n",
      "             5          -0.25940        1.000\n",
      "             6          -0.21380        1.000\n",
      "             7          -0.18183        1.000\n",
      "             8          -0.15817        1.000\n",
      "             9          -0.13996        1.000\n",
      "            10          -0.12550        1.000\n",
      "            11          -0.11376        1.000\n",
      "            12          -0.10402        1.000\n",
      "            13          -0.09582        1.000\n",
      "            14          -0.08882        1.000\n",
      "            15          -0.08278        1.000\n",
      "            16          -0.07750        1.000\n",
      "            17          -0.07286        1.000\n",
      "            18          -0.06874        1.000\n",
      "            19          -0.06507        1.000\n",
      "            20          -0.06176        1.000\n",
      "            21          -0.05878        1.000\n",
      "            22          -0.05607        1.000\n",
      "            23          -0.05360        1.000\n",
      "            24          -0.05134        1.000\n",
      "            25          -0.04927        1.000\n",
      "            26          -0.04735        1.000\n",
      "            27          -0.04558        1.000\n",
      "            28          -0.04394        1.000\n",
      "            29          -0.04241        1.000\n",
      "            30          -0.04098        1.000\n",
      "            31          -0.03965        1.000\n",
      "            32          -0.03840        1.000\n",
      "            33          -0.03723        1.000\n",
      "            34          -0.03612        1.000\n",
      "            35          -0.03508        1.000\n",
      "            36          -0.03410        1.000\n",
      "            37          -0.03318        1.000\n",
      "            38          -0.03230        1.000\n",
      "            39          -0.03146        1.000\n",
      "            40          -0.03067        1.000\n",
      "            41          -0.02992        1.000\n",
      "            42          -0.02921        1.000\n",
      "            43          -0.02852        1.000\n",
      "            44          -0.02787        1.000\n",
      "            45          -0.02725        1.000\n",
      "            46          -0.02665        1.000\n",
      "            47          -0.02609        1.000\n",
      "            48          -0.02554        1.000\n",
      "            49          -0.02502        1.000\n",
      "            50          -0.02451        1.000\n",
      "            51          -0.02403        1.000\n",
      "            52          -0.02357        1.000\n",
      "            53          -0.02312        1.000\n",
      "            54          -0.02269        1.000\n",
      "            55          -0.02228        1.000\n",
      "            56          -0.02188        1.000\n",
      "            57          -0.02150        1.000\n",
      "            58          -0.02112        1.000\n",
      "            59          -0.02077        1.000\n",
      "            60          -0.02042        1.000\n",
      "            61          -0.02008        1.000\n",
      "            62          -0.01976        1.000\n",
      "            63          -0.01945        1.000\n",
      "            64          -0.01914        1.000\n",
      "            65          -0.01885        1.000\n",
      "            66          -0.01856        1.000\n",
      "            67          -0.01828        1.000\n",
      "            68          -0.01801        1.000\n",
      "            69          -0.01775        1.000\n",
      "            70          -0.01750        1.000\n",
      "            71          -0.01725        1.000\n",
      "            72          -0.01701        1.000\n",
      "            73          -0.01678        1.000\n",
      "            74          -0.01655        1.000\n",
      "            75          -0.01633        1.000\n",
      "            76          -0.01612        1.000\n",
      "            77          -0.01591        1.000\n",
      "            78          -0.01570        1.000\n",
      "            79          -0.01550        1.000\n",
      "            80          -0.01531        1.000\n",
      "            81          -0.01512        1.000\n",
      "            82          -0.01494        1.000\n",
      "            83          -0.01476        1.000\n",
      "            84          -0.01458        1.000\n",
      "            85          -0.01441        1.000\n",
      "            86          -0.01424        1.000\n",
      "            87          -0.01408        1.000\n",
      "            88          -0.01392        1.000\n",
      "            89          -0.01376        1.000\n",
      "            90          -0.01361        1.000\n",
      "            91          -0.01346        1.000\n",
      "            92          -0.01331        1.000\n",
      "            93          -0.01317        1.000\n",
      "            94          -0.01303        1.000\n",
      "            95          -0.01289        1.000\n",
      "            96          -0.01276        1.000\n",
      "            97          -0.01263        1.000\n",
      "            98          -0.01250        1.000\n",
      "            99          -0.01237        1.000\n",
      "         Final          -0.01225        1.000\n",
      "Original Word: doors\n",
      "POS Tags:  [('doors', 'NNS')]\n",
      "==================================================\n",
      "Original Word: pillows\n",
      "POS Tags:  [('pillows', 'NNS')]\n",
      "==================================================\n",
      "Original Word: cars\n",
      "POS Tags:  [('cars', 'NNS')]\n",
      "==================================================\n",
      "Original Word: cats\n",
      "POS Tags:  [('cats', 'NNS')]\n",
      "==================================================\n",
      "Original Word: Books\n",
      "POS Tags:  [('Books', 'NNS')]\n",
      "==================================================\n",
      "Original Word: Houses\n",
      "POS Tags:  [('Houses', 'NNS')]\n",
      "==================================================\n",
      "Original Word: Trees\n",
      "POS Tags:  [('Trees', 'NNS')]\n",
      "==================================================\n",
      "Original Word: Computers\n",
      "POS Tags:  [('Computers', 'NNS')]\n",
      "==================================================\n",
      "Original Word: Flowers\n",
      "POS Tags:  [('Flowers', 'NNS')]\n",
      "==================================================\n",
      "Original Word: Birds\n",
      "POS Tags:  [('Birds', 'NNS')]\n",
      "==================================================\n",
      "Original Word: Children\n",
      "POS Tags:  [('Children', 'NNS')]\n",
      "==================================================\n",
      "Original Word: Chairs\n",
      "POS Tags:  [('Chairs', 'NNS')]\n",
      "==================================================\n",
      "Original Word: Keys\n",
      "POS Tags:  [('Keys', 'NNS')]\n",
      "==================================================\n",
      "Original Word: Cups\n",
      "POS Tags:  [('Cups', 'NNS')]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.tag import ClassifierBasedPOSTagger\n",
    "from nltk.classify import MaxentClassifier\n",
    "\n",
    "# Training sentences for the custom POS tagger\n",
    "sent_train = [\n",
    "    [(\"cars\", \"NNS\"), (\"are\", \"VBP\"), (\"parked\", \"VBN\"), (\"in\", \"IN\"), (\"the\", \"DT\"), (\"garage\", \"NN\")],\n",
    "    [(\"cats\", \"NNS\"), (\"are\", \"VBP\"), (\"playing\", \"VBG\"), (\"in\", \"IN\"), (\"the\", \"DT\"), (\"garden\", \"NN\")],\n",
    "    [(\"doors\", \"NNS\"), (\"are\", \"VBP\"), (\"closed\", \"VBN\")],\n",
    "    [(\"pillows\", \"NNS\"), (\"are\", \"VBP\"), (\"on\", \"IN\"), (\"the\", \"DT\"), (\"sofa\", \"NN\")]\n",
    "]\n",
    "\n",
    "# I used the nltk library and didn't use the gensim library \n",
    "# due to a version issue with gensim.\n",
    "\n",
    "# Train the classifier based on Maxent\n",
    "classifier_tagger = ClassifierBasedPOSTagger(train=sent_train, classifier_builder=MaxentClassifier.train)\n",
    "\n",
    "# The classifier is built using the Maximum Entropy method, which is a \n",
    "# widely used machine learning algorithm for classification. \n",
    "# The 'MaxentClassifier.train' function is used to train the classifier \n",
    "# using the provided training data (sent_train). \n",
    "\n",
    "test = [\n",
    "    \"doors\",\n",
    "    \"pillows\",\n",
    "    \"cars\",\n",
    "    \"cats\",\n",
    "    \"Books\",\n",
    "    \"Houses\",\n",
    "    \"Trees\",\n",
    "    \"Computers\",\n",
    "    \"Flowers\",\n",
    "    \"Birds\",\n",
    "    \"Children\",\n",
    "    \"Chairs\",\n",
    "    \"Keys\",\n",
    "    \"Cups\"\n",
    "]\n",
    "\n",
    "# Test the POS tagger\n",
    "for word in test:\n",
    "    words = word_tokenize(word)\n",
    "    pos_tags = classifier_tagger.tag(words)\n",
    "    print(\"Original Word:\", word)\n",
    "    print(\"POS Tags: \", pos_tags)\n",
    "    print(\"=\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
